{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9de79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from io import BytesIO\n",
    "import xlsxwriter\n",
    "from dataiku.core.sql import SQLExecutor2\n",
    "import time\n",
    "from zipfile import Zipfile\n",
    "from os.path import basename\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# storing proj kep in variable\n",
    "client = dataiku.api_client()\n",
    "proj = client.get_default_project()\n",
    "project = proj.get_summary()['projectKey']\n",
    "\n",
    "# Read recipe inputs\n",
    "mydataset = dataiku.Dataset(\"Table name\")\n",
    "mydataset_df = mydataset.get_dataframe()\n",
    "\n",
    "#-------Or----------\n",
    "mydataset = dataiku.Dataset(\"Table name\")\n",
    "executor = SQLExecutor2(dataset = mydataset)\n",
    "tb_tracker_df = executor.query_to_df(\n",
    "\"\"\"\n",
    "select \"company code\", \"year\" from \"AN@LYTIC_PG_PR...%s\" \"\"\"  %(project) + \"\"\".\"%s_table name\" \n",
    "where \"year\" in (CAST((Year(GETDATE())) AS int), CAST((Year(GETDATE())-1) AS int))\n",
    "group by \"company code\", \"year\"\n",
    "\"\"\" %(project))\n",
    "\n",
    "# appending dfs to create one and drop duplicate values\n",
    "frames = [frame1, frame2, frame3]\n",
    "compcd = pd.concat(frames).drop_duplicates()\n",
    "\n",
    "# checking for user inputs\n",
    "input_var = dataiku.get_custom_variables()['Variable name']\n",
    "elements = input_var.split(',')\n",
    "user_input = []\n",
    "for i in elements:\n",
    "    if len(i.strip())>0:\n",
    "        user_input.append(i.strip())\n",
    "        \n",
    "# filtering dataset based on user inputs\n",
    "if len(''.join(user_input))>0:\n",
    "    mydataset_df_selected = mydataset_df[mydataset_df['Col Name'].isin(user_input)]\n",
    "else:\n",
    "    mydataset_df_selected = mydataset_df\n",
    "    \n",
    "# initializing counter variables\n",
    "i = 0\n",
    "j = 1\n",
    "# Declaring Sharepoint output folder\n",
    "out_folder = dataiku.folder(\"xxyyzzaabb\")\n",
    "\n",
    "# Querying the tables with company code and year combo\n",
    "\n",
    "for comp, year_df in enumerate(mydataset_df_selected.values):\n",
    "    mydataset = dataiku.Dataset(\" Table name\")\n",
    "    executor = SQLExecutor2(dataset = mydataset)\n",
    "    sqlstring = (\"\"\"select * from \"AN@LYTICS_PG_PR_%s\" \"\"\" %(project) + \"\"\".\"%s_Table name\" where\"\"\" %(project) + \"\"\" \\\"COMPANY CODE\\\" = %d\"\"\" %(year_df[0]) + \"\"\" and \\\"YEAR\\\" = %d\"\"\" %(year_df[1]))\n",
    "    Table_df = executor.query_to_df(sqlstring)\n",
    "\n",
    "    mydataset1 = dataiku.Dataset(\"Table name 1\")\n",
    "    executor = SQLExecutor2(dataset = mydataset1)\n",
    "    sqlstring = (\"\"\"select * from \"AN@LYTICS_PG_PR_%s\" \"\"\" %(project) + \"\"\".\"%s_Table name1\" where\"\"\" %(project) + \"\"\" \\\"COMPANY CODE\\\" = %d\"\"\" %(year_df[0]) + \"\"\" and \\\"YEAR\\\" = %d\"\"\" %(year_df[1]))\n",
    "    Table_df1 = executor.query_to_df(sqlstring)\n",
    "\n",
    "    mydataset2 = dataiku.Dataset(\"Table name 2\")\n",
    "    executor = SQLExecutor2(dataset = mydataset2)\n",
    "    sqlstring = (\"\"\"select * from \"AN@LYTICS_PG_PR_%s\" \"\"\" %(project) + \"\"\".\"%s_Table name2\" where\"\"\" %(project) + \"\"\" \\\"COMPANY CODE\\\" = %d\"\"\" %(year_df[0]) + \"\"\" and \\\"YEAR\\\" = %d\"\"\" %(year_df[1]))\n",
    "    Table_df2 = executor.query_to_df(sqlstring)\n",
    "\n",
    "    company_code = year_df[0]\n",
    "    fisc@lyear_current = year_df[1]\n",
    "    \n",
    "    # Storing the dataframes in a dictionary\n",
    "    frames = {}\n",
    "    if Table_df.empty:\n",
    "        pass\n",
    "    else:\n",
    "        col_names = ['col1', 'col2', '....', 'colX']\n",
    "        table_df = Table_df[col_names].copy()\n",
    "        frames['Table_df'] = table_df\n",
    "        \n",
    "    if Table_df1.empty:\n",
    "        pass\n",
    "    else:\n",
    "        col_names = ['col1', 'col2', '....', 'colX']\n",
    "        table_df1 = Table_df1[col_names].copy()\n",
    "        frames['Table_df1'] = table_df1\n",
    "        \n",
    "    if Table_df2.empty:\n",
    "        pass\n",
    "    else:\n",
    "        col_names = ['col1', 'col2', '....', 'colX']\n",
    "        table_df2 = Table_df2[col_names].copy()\n",
    "# Iterating through the dataframe 5,00,000 rows at a time\n",
    "        CHUNK_SIZE = 500000\n",
    "        iterator =1\n",
    "        for chunk_num in range(len(table_df2) // CHUNK_SIZE +1):\n",
    "            start_index = chunk_num*CHUNK_SIZE\n",
    "            end_index = min(chunk_num*CHUNK_SIZE + CHUNK_SIZE, len(table_df2))\n",
    "            chunk = table_df2[start_index:end_index]\n",
    "            frames['Table_df2' + str(iterator)] = chunk\n",
    "            iterator += 1\n",
    "\n",
    "# loop to iterate through the dict and create formatted excel output\n",
    "    stream = BytesIO()\n",
    "    excel_writer = pd.ExcelWriter(stream, engine = 'xlsxwriter')\n",
    "    \n",
    "    for keys, filter_df in frames.items():\n",
    "        \n",
    "        filter_df.to_excel(excel_writer, sheet_name = keys, index = False)\n",
    "        workbook = excel_writer.book\n",
    "        worksheet = excel_writer.sheets[keys]\n",
    "        \n",
    "        # formatting the excel\n",
    "        header_format = workbook.add_format({'bg_color' : '#e6e6e6',\n",
    "                                            'font_color': 'black',\n",
    "                                            'font_size': 'Arial',\n",
    "                                            'bold': True})\n",
    "        \n",
    "        right_header_format = workbook.add_format({'bg_color' : '#e6e6e6',\n",
    "                                            'font_color': 'black',\n",
    "                                            'font_size': 'Arial',\n",
    "                                            'align' : 'right',\n",
    "                                            'bold': True})\n",
    "        number_format = workbook.add_format({'num_format': '###,###,###,###,###0'})\n",
    "        number_rows = len(filter_df.index) + 1\n",
    "        for col_header_num, header_value in enumerate(filter_df.columns.values):\n",
    "            if (keys == 'Table_df'):\n",
    "                num_header1 = ['Col1', 'Col2' , '...', 'colX']\n",
    "                if (header_value in num_header1):\n",
    "                    worksheet.write(0, col_header_num, header_value, right_header_format)\n",
    "                else:\n",
    "                    workshet.write(0, col_header_num, header_value, header_format)\n",
    "            else (keys in LocalOnly_sheet_names):\n",
    "                if(col_header_num == 26):\n",
    "                    worksheet.write(0, col_header_num, header_value, right_header_format)\n",
    "                else:\n",
    "                    worksheet.write(0, col_header_num, header_value, header_format)\n",
    "                    \n",
    "        if (keys == 'Table_df'):\n",
    "            worksheet.conditional_format(\"$T$2:$Z%d\" % (number_rows),\n",
    "                                         {'type': 'cell',\n",
    "                                         'criteria': '!=',\n",
    "                                         'value': 0,\n",
    "                                         \"format\": number_format})\n",
    "        if (keys.startswith('Table_df2')):\n",
    "            worksheet.conditional_format(\"$C$2:$P%d\" % (number_rows),\n",
    "                                         {'type': 'cell',\n",
    "                                         'criteria': '!=',\n",
    "                                         'value': 0,\n",
    "                                         \"format\": number_format})            \n",
    "            \n",
    "        measurer = np.vectorize(len)\n",
    "        for i in range(filter_df.shepe[1]):\n",
    "            # find width of i column\n",
    "            columnNamewidth = len(filter_df.columns[i])\n",
    "            # find max width of values in i column\n",
    "            maxColumnWidth = measurer(filter_df.iloc[:, i].values.astype(str)).max(axis = 0)\n",
    "            # set column width only for that column, +2 for padding\n",
    "            worksheet.set_column(i, i, max(columnNameWidth, maxColumnWidth)+2)\n",
    "    excel_writer.save()\n",
    "    stream.seek(0)\n",
    "    # End of for loop\n",
    "    \n",
    "    date_stamp = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "    with out_folder.get_writer(\"%s/%s/%d/Folder_Name/%s_%d_Table Name_%s.xlsx\" %(currcountry, company, fisc@lyrcurr, company,fisc@lyrcurr, date_stamp)) as writer:\n",
    "        successful = False\n",
    "        attempts = 0\n",
    "        while not successful and attempts < 3:\n",
    "            attempts += 1\n",
    "            try:\n",
    "                writer.writer(stream.read())\n",
    "                successful = True\n",
    "            except:\n",
    "                time.sleep(5)\n",
    "\n",
    "'''\n",
    "# Reading the local folder to zip all the files\n",
    "files = out_folder.list_paths_in_partition()\n",
    "filesIndex = 0\n",
    "filesNameIndex = 1\n",
    "files2 = files.copy()\n",
    "\n",
    "#zipping the contents of the folder\n",
    "with ZipFile(out_folder.get_path()+ '/Name of the file.zip', 'w', zipfile.ZIP_DEFLATED) as zipobj:\n",
    "    while(filesIndex < len(files2)):\n",
    "        zipObj.write(out_folder.get_path() + files2[filesIndex], files2[filesIndex])\n",
    "        filesIndex += 1\n",
    "        \n",
    "# Copying the local folder to Share point\n",
    "client = dataiku.api_client()\n",
    "project = client.get_project(dataiku.default_project_key())\n",
    "\n",
    "source_folder = project.get_managed_folder(\"zzxxxccvv\")\n",
    "target_folder = project.get_managed_folder(\"SP folder name\")\n",
    "\n",
    "future = source_folder.copy_to(target_folder, write_mode = 'OVERWRITE')\n",
    "future.wait_for_result()\n",
    "\n",
    "# Check for zip folder and then move just that\n",
    "x=0\n",
    "for paths[x] in paths:\n",
    "    if re.search(\"Local Folder\", paths[x]):\n",
    "        with input_folder.get_download_stream(paths[x]) as f:\n",
    "            data = f.read()\n",
    "        with output_folder1.get_writer(paths[x]) as w:\n",
    "            w.write(data)\n",
    "    x += 1\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
